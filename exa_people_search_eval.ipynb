{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 - Install / imports + env load\n",
    "# Fresh venv? Uncomment once:\n",
    "# %pip install -r requirements.txt\n",
    "\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import sqlite3\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, Optional, List, Tuple\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # loads .env from the notebook working directory\n",
    "\n",
    "EXA_API_KEY = (os.getenv(\"EXA_API_KEY\") or \"\").strip()\n",
    "EXA_SMOKE_NO_NETWORK = (os.getenv(\"EXA_SMOKE_NO_NETWORK\") or \"0\").strip() == \"1\"\n",
    "\n",
    "if not EXA_API_KEY and not EXA_SMOKE_NO_NETWORK:\n",
    "    raise RuntimeError(\n",
    "        \"Missing EXA_API_KEY. Create .env from .env.example and set EXA_API_KEY=...\\n\"\n",
    "        \"For local smoke runs without network/API billing, set EXA_SMOKE_NO_NETWORK=1.\"\n",
    "    )\n",
    "\n",
    "if EXA_SMOKE_NO_NETWORK:\n",
    "    print(\"Smoke mode enabled (no network): using local mock Exa responses.\")\n",
    "else:\n",
    "    print(\"Loaded EXA_API_KEY from .env (not printing it).\")\n"
   ],
   "id": "cell01"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 - Config\n",
    "\n",
    "CONFIG = {\n",
    "    # Exa search request settings\n",
    "    \"exa_endpoint\": \"https://api.exa.ai/search\",\n",
    "    \"search_type\": \"auto\",        # auto | neural | fast | deep\n",
    "    \"category\": \"people\",         # keep focused on professional people profiles\n",
    "    \"num_results\": 5,               # cheap default\n",
    "    \"user_location\": \"US\",\n",
    "\n",
    "    # Contents toggles (each enabled content type can add per-result cost)\n",
    "    \"use_text\": False,              # expensive; leave off for first pass\n",
    "    \"use_highlights\": True,         # recommended cheap default\n",
    "    \"highlights_per_url\": 1,\n",
    "    \"highlight_num_sentences\": 2,\n",
    "    \"use_summary\": False,           # keep off unless needed\n",
    "\n",
    "    # Optional domain controls (leave empty unless you want stricter scoping)\n",
    "    \"include_domains\": [],\n",
    "    \"exclude_domains\": [],\n",
    "\n",
    "    # Safety / moderation\n",
    "    \"moderation\": True,\n",
    "    \"redact_emails_phones\": True,\n",
    "\n",
    "    # Budget (hard stop for uncached calls only)\n",
    "    \"budget_cap_usd\": 7.50,\n",
    "\n",
    "    # Cache (prevents re-billing on repeat runs)\n",
    "    \"sqlite_path\": \"exa_cache.sqlite\",\n",
    "    \"cache_ttl_hours\": 24 * 30,  # 30 days\n",
    "}\n",
    "\n",
    "# Pricing assumptions (keep aligned with Exa pricing page before production use)\n",
    "PRICING = {\n",
    "    # Search request pricing tier by requested result count\n",
    "    \"search_1_25\": 0.005,\n",
    "    \"search_26_100\": 0.025,\n",
    "\n",
    "    # Content extraction pricing (per page, per content type)\n",
    "    \"content_text_per_page\": 0.001,\n",
    "    \"content_highlights_per_page\": 0.001,\n",
    "    \"content_summary_per_page\": 0.001,\n",
    "}\n",
    "\n",
    "print(\"CONFIG loaded\")\n",
    "print(json.dumps(CONFIG, indent=2))\n"
   ],
   "id": "cell02"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 - Exa call wrapper\n",
    "\n",
    "def _canonical_json(obj: Any) -> str:\n",
    "    return json.dumps(obj, sort_keys=True, ensure_ascii=False, separators=(\",\", \":\"))\n",
    "\n",
    "\n",
    "def _sha256(s: str) -> str:\n",
    "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "def _estimate_cost_from_pricing(payload: Dict[str, Any], num_results: int) -> float:\n",
    "    search_cost = PRICING[\"search_1_25\"] if num_results <= 25 else PRICING[\"search_26_100\"]\n",
    "\n",
    "    contents_cost = 0.0\n",
    "    contents = payload.get(\"contents\") or {}\n",
    "    if contents.get(\"text\") is True:\n",
    "        contents_cost += num_results * PRICING[\"content_text_per_page\"]\n",
    "    if isinstance(contents.get(\"highlights\"), dict):\n",
    "        contents_cost += num_results * PRICING[\"content_highlights_per_page\"]\n",
    "    if isinstance(contents.get(\"summary\"), dict):\n",
    "        contents_cost += num_results * PRICING[\"content_summary_per_page\"]\n",
    "\n",
    "    return round(search_cost + contents_cost, 6)\n",
    "\n",
    "\n",
    "EMAIL_RE = re.compile(r\"\\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,}\\b\", re.IGNORECASE)\n",
    "PHONE_RE = re.compile(r\"(\\+?\\d[\\d\\-\\s().]{7,}\\d)\")\n",
    "STREETISH_RE = re.compile(\n",
    "    r\"\\b\\d{1,5}\\s+[A-Za-z0-9.\\-]+\\s+(Street|St|Avenue|Ave|Road|Rd|Boulevard|Blvd|Lane|Ln|Drive|Dr|Court|Ct|Circle|Cir|Way)\\b\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "\n",
    "def redact_text(s: Optional[str]) -> Optional[str]:\n",
    "    if not s or not CONFIG.get(\"redact_emails_phones\", True):\n",
    "        return s\n",
    "    s = EMAIL_RE.sub(\"[REDACTED_EMAIL]\", s)\n",
    "    s = PHONE_RE.sub(\"[REDACTED_PHONE]\", s)\n",
    "    s = STREETISH_RE.sub(\"[REDACTED_ADDRESS]\", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def extract_preview(result: Dict[str, Any], max_chars: int = 280) -> str:\n",
    "    highlights = result.get(\"highlights\")\n",
    "    if isinstance(highlights, list) and highlights:\n",
    "        return redact_text(\" | \".join(str(x) for x in highlights)) or \"\"\n",
    "    text = result.get(\"text\")\n",
    "    if isinstance(text, str) and text:\n",
    "        return (redact_text(text[:max_chars]) or \"\")\n",
    "    summary = result.get(\"summary\")\n",
    "    if isinstance(summary, str):\n",
    "        return (redact_text(summary[:max_chars]) or \"\")\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExaCallMeta:\n",
    "    cache_hit: bool\n",
    "    request_hash: str\n",
    "    request_payload: Dict[str, Any]\n",
    "    estimated_cost_usd: float\n",
    "    actual_cost_usd: Optional[float]\n",
    "    request_id: Optional[str]\n",
    "    resolved_search_type: Optional[str]\n",
    "    created_at_utc: str\n",
    "\n",
    "\n",
    "def build_exa_payload(query: str, *, num_results: Optional[int] = None) -> Dict[str, Any]:\n",
    "    num_results = int(num_results or CONFIG[\"num_results\"])\n",
    "    payload: Dict[str, Any] = {\n",
    "        \"query\": query,\n",
    "        \"type\": CONFIG[\"search_type\"],\n",
    "        \"category\": CONFIG[\"category\"],\n",
    "        \"numResults\": num_results,\n",
    "        \"userLocation\": CONFIG[\"user_location\"],\n",
    "        \"moderation\": CONFIG[\"moderation\"],\n",
    "    }\n",
    "\n",
    "    if CONFIG[\"include_domains\"]:\n",
    "        payload[\"includeDomains\"] = CONFIG[\"include_domains\"]\n",
    "    if CONFIG[\"exclude_domains\"]:\n",
    "        payload[\"excludeDomains\"] = CONFIG[\"exclude_domains\"]\n",
    "\n",
    "    contents: Dict[str, Any] = {}\n",
    "    if CONFIG[\"use_text\"]:\n",
    "        contents[\"text\"] = True\n",
    "    if CONFIG[\"use_highlights\"]:\n",
    "        contents[\"highlights\"] = {\n",
    "            \"highlightsPerUrl\": CONFIG[\"highlights_per_url\"],\n",
    "            \"numSentences\": CONFIG[\"highlight_num_sentences\"],\n",
    "        }\n",
    "    if CONFIG[\"use_summary\"]:\n",
    "        contents[\"summary\"] = {\n",
    "            \"query\": \"Summarize the person's professional background and insurance/CAT relevance.\"\n",
    "        }\n",
    "\n",
    "    if contents:\n",
    "        payload[\"contents\"] = contents\n",
    "\n",
    "    return payload\n",
    "\n",
    "\n",
    "def exa_search_people(query: str, *, num_results: Optional[int] = None) -> Tuple[Dict[str, Any], ExaCallMeta]:\n",
    "    payload = build_exa_payload(query, num_results=num_results)\n",
    "    estimated_cost = _estimate_cost_from_pricing(payload, int(payload[\"numResults\"]))\n",
    "\n",
    "    response_json, cache_hit = cache_get_or_set(payload, estimated_cost)  # defined in Cell 4\n",
    "\n",
    "    actual_cost = None\n",
    "    if isinstance(response_json, dict):\n",
    "        cost = response_json.get(\"costDollars\")\n",
    "        if isinstance(cost, dict) and isinstance(cost.get(\"total\"), (int, float)):\n",
    "            actual_cost = float(cost[\"total\"])\n",
    "\n",
    "    meta = ExaCallMeta(\n",
    "        cache_hit=cache_hit,\n",
    "        request_hash=_sha256(_canonical_json(payload)),\n",
    "        request_payload=payload,\n",
    "        estimated_cost_usd=estimated_cost,\n",
    "        actual_cost_usd=actual_cost,\n",
    "        request_id=response_json.get(\"requestId\") if isinstance(response_json, dict) else None,\n",
    "        resolved_search_type=response_json.get(\"resolvedSearchType\") if isinstance(response_json, dict) else None,\n",
    "        created_at_utc=datetime.now(timezone.utc).isoformat(),\n",
    "    )\n",
    "    return response_json, meta\n"
   ],
   "id": "cell03"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 - Cache wrapper (sqlite) + budget enforcement\n",
    "\n",
    "def _db() -> sqlite3.Connection:\n",
    "    conn = sqlite3.connect(CONFIG[\"sqlite_path\"])\n",
    "    conn.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS exa_cache (\n",
    "            request_hash TEXT PRIMARY KEY,\n",
    "            request_json TEXT NOT NULL,\n",
    "            response_json TEXT NOT NULL,\n",
    "            estimated_cost_usd REAL NOT NULL,\n",
    "            actual_cost_usd REAL,\n",
    "            created_at_utc TEXT NOT NULL\n",
    "        )\n",
    "        \"\"\"\n",
    "    )\n",
    "    conn.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS exa_ledger (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            request_hash TEXT NOT NULL,\n",
    "            query TEXT NOT NULL,\n",
    "            cache_hit INTEGER NOT NULL,\n",
    "            estimated_cost_usd REAL NOT NULL,\n",
    "            actual_cost_usd REAL,\n",
    "            created_at_utc TEXT NOT NULL\n",
    "        )\n",
    "        \"\"\"\n",
    "    )\n",
    "    conn.commit()\n",
    "    return conn\n",
    "\n",
    "\n",
    "def cache_lookup(request_hash: str) -> Optional[Dict[str, Any]]:\n",
    "    conn = _db()\n",
    "    try:\n",
    "        row = conn.execute(\n",
    "            \"SELECT response_json, created_at_utc FROM exa_cache WHERE request_hash = ?\",\n",
    "            (request_hash,),\n",
    "        ).fetchone()\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "    if not row:\n",
    "        return None\n",
    "\n",
    "    response_json, created_at_utc = row\n",
    "    try:\n",
    "        created = datetime.fromisoformat(str(created_at_utc).replace(\"Z\", \"+00:00\"))\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "    age_hours = (datetime.now(timezone.utc) - created).total_seconds() / 3600\n",
    "    if age_hours > float(CONFIG[\"cache_ttl_hours\"]):\n",
    "        return None\n",
    "\n",
    "    return json.loads(response_json)\n",
    "\n",
    "\n",
    "def cache_store(request_hash: str, request_payload: Dict[str, Any], response_json: Dict[str, Any], estimated_cost: float) -> None:\n",
    "    actual_cost = None\n",
    "    if isinstance(response_json, dict):\n",
    "        cost = response_json.get(\"costDollars\")\n",
    "        if isinstance(cost, dict) and isinstance(cost.get(\"total\"), (int, float)):\n",
    "            actual_cost = float(cost[\"total\"])\n",
    "\n",
    "    conn = _db()\n",
    "    try:\n",
    "        conn.execute(\n",
    "            \"INSERT OR REPLACE INTO exa_cache (request_hash, request_json, response_json, estimated_cost_usd, actual_cost_usd, created_at_utc) VALUES (?, ?, ?, ?, ?, ?)\",\n",
    "            (\n",
    "                request_hash,\n",
    "                _canonical_json(request_payload),\n",
    "                _canonical_json(response_json),\n",
    "                float(estimated_cost),\n",
    "                actual_cost,\n",
    "                datetime.now(timezone.utc).isoformat(),\n",
    "            ),\n",
    "        )\n",
    "        conn.commit()\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "def ledger_add(request_hash: str, query: str, cache_hit: bool, estimated_cost: float, actual_cost: Optional[float]) -> None:\n",
    "    conn = _db()\n",
    "    try:\n",
    "        conn.execute(\n",
    "            \"INSERT INTO exa_ledger (request_hash, query, cache_hit, estimated_cost_usd, actual_cost_usd, created_at_utc) VALUES (?, ?, ?, ?, ?, ?)\",\n",
    "            (\n",
    "                request_hash,\n",
    "                query,\n",
    "                1 if cache_hit else 0,\n",
    "                float(estimated_cost),\n",
    "                None if actual_cost is None else float(actual_cost),\n",
    "                datetime.now(timezone.utc).isoformat(),\n",
    "            ),\n",
    "        )\n",
    "        conn.commit()\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "def ledger_summary() -> pd.DataFrame:\n",
    "    conn = _db()\n",
    "    try:\n",
    "        df = pd.read_sql_query(\"SELECT * FROM exa_ledger ORDER BY id ASC\", conn)\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "    expected_cols = [\n",
    "        \"id\",\n",
    "        \"request_hash\",\n",
    "        \"query\",\n",
    "        \"cache_hit\",\n",
    "        \"estimated_cost_usd\",\n",
    "        \"actual_cost_usd\",\n",
    "        \"created_at_utc\",\n",
    "    ]\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=expected_cols)\n",
    "    return df\n",
    "\n",
    "\n",
    "def spend_so_far() -> Dict[str, float]:\n",
    "    df = ledger_summary()\n",
    "    if df.empty:\n",
    "        return {\n",
    "            \"request_count\": 0,\n",
    "            \"cache_hits\": 0,\n",
    "            \"uncached_calls\": 0,\n",
    "            \"spent_usd\": 0.0,\n",
    "            \"avg_cost_per_uncached_query\": 0.0,\n",
    "        }\n",
    "\n",
    "    billable = []\n",
    "    for _, row in df.iterrows():\n",
    "        if int(row[\"cache_hit\"]) == 1:\n",
    "            billable.append(0.0)\n",
    "        elif pd.notna(row[\"actual_cost_usd\"]):\n",
    "            billable.append(float(row[\"actual_cost_usd\"]))\n",
    "        else:\n",
    "            billable.append(float(row[\"estimated_cost_usd\"]))\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"billable_cost_usd\"] = billable\n",
    "    uncached_mask = df[\"cache_hit\"].astype(int) == 0\n",
    "    uncached_count = int(uncached_mask.sum())\n",
    "    avg_uncached = float(df.loc[uncached_mask, \"billable_cost_usd\"].mean()) if uncached_count else 0.0\n",
    "\n",
    "    return {\n",
    "        \"request_count\": int(len(df)),\n",
    "        \"cache_hits\": int((df[\"cache_hit\"].astype(int) == 1).sum()),\n",
    "        \"uncached_calls\": uncached_count,\n",
    "        \"spent_usd\": round(float(df[\"billable_cost_usd\"].sum()), 6),\n",
    "        \"avg_cost_per_uncached_query\": round(avg_uncached, 6),\n",
    "    }\n",
    "\n",
    "\n",
    "def enforce_budget(next_estimated_cost: float) -> None:\n",
    "    metrics = spend_so_far()\n",
    "    projected_spend = float(metrics[\"spent_usd\"]) + float(next_estimated_cost)\n",
    "    if projected_spend > float(CONFIG[\"budget_cap_usd\"]):\n",
    "        raise RuntimeError(\n",
    "            \"Budget cap exceeded before uncached Exa call.\\n\"\n",
    "            f\"Spent so far: ${metrics['spent_usd']:.4f}\\n\"\n",
    "            f\"Next call estimate: ${float(next_estimated_cost):.4f}\\n\"\n",
    "            f\"Cap: ${float(CONFIG['budget_cap_usd']):.2f}\\n\"\n",
    "            \"Lower num_results and/or disable text/summary, or rerun cached queries.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def _mock_exa_response(payload: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    query = str(payload.get(\"query\") or \"\")\n",
    "    num_results = int(payload.get(\"numResults\") or 5)\n",
    "    contents = payload.get(\"contents\") or {}\n",
    "    wants_highlights = isinstance(contents.get(\"highlights\"), dict)\n",
    "    wants_text = contents.get(\"text\") is True\n",
    "    wants_summary = isinstance(contents.get(\"summary\"), dict)\n",
    "\n",
    "    slug = _sha256(query)[:8]\n",
    "    results: List[Dict[str, Any]] = []\n",
    "    for i in range(num_results):\n",
    "        title = f\"Mock Professional Result {i+1} - CAT loss / insurance expert\"\n",
    "        url = f\"https://www.linkedin.com/in/mock-{slug}-{i+1}\"\n",
    "        item: Dict[str, Any] = {\n",
    "            \"id\": f\"mock-{slug}-{i+1}\",\n",
    "            \"title\": title,\n",
    "            \"url\": url,\n",
    "        }\n",
    "        if wants_highlights:\n",
    "            item[\"highlights\"] = [\n",
    "                f\"Mock highlight for query: {query}. Public professional profile for insurance litigation and expert witness context.\"\n",
    "            ]\n",
    "            item[\"highlightScores\"] = [0.99]\n",
    "        if wants_text:\n",
    "            item[\"text\"] = (\n",
    "                \"Mock text body. Public/professional info only. No personal addresses or contact harvesting. \"\n",
    "                f\"Query context: {query}.\"\n",
    "            )\n",
    "        if wants_summary:\n",
    "            item[\"summary\"] = \"Mock summary: relevant professional background for insurance/CAT-loss workflow evaluation.\"\n",
    "        results.append(item)\n",
    "\n",
    "    return {\n",
    "        \"requestId\": f\"smoke-{slug}\",\n",
    "        \"resolvedSearchType\": str(payload.get(\"type\") or \"auto\"),\n",
    "        \"results\": results,\n",
    "        \"costDollars\": {\n",
    "            \"search\": 0.0,\n",
    "            \"contents\": 0.0,\n",
    "            \"total\": 0.0,\n",
    "        },\n",
    "        \"_smokeMode\": True,\n",
    "    }\n",
    "\n",
    "\n",
    "def exa_http_call(payload: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    if EXA_SMOKE_NO_NETWORK:\n",
    "        return _mock_exa_response(payload)\n",
    "\n",
    "    if not EXA_API_KEY:\n",
    "        raise RuntimeError(\"Missing EXA_API_KEY for live Exa request.\")\n",
    "\n",
    "    headers = {\n",
    "        \"x-api-key\": EXA_API_KEY,\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    response = requests.post(CONFIG[\"exa_endpoint\"], headers=headers, json=payload, timeout=60)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def cache_get_or_set(payload: Dict[str, Any], estimated_cost: float) -> Tuple[Dict[str, Any], bool]:\n",
    "    request_hash = _sha256(_canonical_json(payload))\n",
    "    cached = cache_lookup(request_hash)\n",
    "    if cached is not None:\n",
    "        cached_cost = None\n",
    "        if isinstance(cached, dict):\n",
    "            cost = cached.get(\"costDollars\")\n",
    "            if isinstance(cost, dict) and isinstance(cost.get(\"total\"), (int, float)):\n",
    "                cached_cost = float(cost[\"total\"])\n",
    "        ledger_add(\n",
    "            request_hash=request_hash,\n",
    "            query=str(payload.get(\"query\") or \"\"),\n",
    "            cache_hit=True,\n",
    "            estimated_cost=estimated_cost,\n",
    "            actual_cost=cached_cost,\n",
    "        )\n",
    "        return cached, True\n",
    "\n",
    "    enforce_budget(estimated_cost)\n",
    "    response_json = exa_http_call(payload)\n",
    "    cache_store(request_hash, payload, response_json, estimated_cost)\n",
    "\n",
    "    actual_cost = None\n",
    "    if isinstance(response_json, dict):\n",
    "        cost = response_json.get(\"costDollars\")\n",
    "        if isinstance(cost, dict) and isinstance(cost.get(\"total\"), (int, float)):\n",
    "            actual_cost = float(cost[\"total\"])\n",
    "\n",
    "    ledger_add(\n",
    "        request_hash=request_hash,\n",
    "        query=str(payload.get(\"query\") or \"\"),\n",
    "        cache_hit=False,\n",
    "        estimated_cost=estimated_cost,\n",
    "        actual_cost=actual_cost,\n",
    "    )\n",
    "    return response_json, False\n",
    "\n",
    "\n",
    "print(\"Cache ready:\", CONFIG[\"sqlite_path\"])\n",
    "print(\"Ledger metrics:\", spend_so_far())\n"
   ],
   "id": "cell04"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 - Single query demo\n",
    "\n",
    "demo_query = \"Florida property insurance attorney hurricane Ian appraisal dispute site:linkedin.com\"\n",
    "resp_demo, meta_demo = exa_search_people(demo_query, num_results=CONFIG[\"num_results\"])\n",
    "\n",
    "print(\"Single-query demo\")\n",
    "print(\"  cache_hit:\", meta_demo.cache_hit)\n",
    "print(\"  estimated_cost_usd:\", meta_demo.estimated_cost_usd)\n",
    "print(\"  actual_cost_usd:\", meta_demo.actual_cost_usd)\n",
    "print(\"  request_id:\", meta_demo.request_id)\n",
    "print(\"  resolved_search_type:\", meta_demo.resolved_search_type)\n",
    "\n",
    "results_demo = resp_demo.get(\"results\", []) if isinstance(resp_demo, dict) else []\n",
    "rows_demo = []\n",
    "for result in results_demo[: int(CONFIG[\"num_results\"])]:\n",
    "    rows_demo.append(\n",
    "        {\n",
    "            \"title\": redact_text(result.get(\"title\")),\n",
    "            \"url\": result.get(\"url\"),\n",
    "            \"preview\": extract_preview(result, max_chars=280),\n",
    "            \"highlightScores\": result.get(\"highlightScores\"),\n",
    "        }\n",
    "    )\n",
    "\n",
    "df_demo = pd.DataFrame(rows_demo)\n",
    "print(\"Rows returned:\", len(df_demo))\n",
    "df_demo\n"
   ],
   "id": "cell05"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 - Batch query test suite (insurance / CAT)\n",
    "\n",
    "# Swap this list to evaluate other workflows while keeping the rest of the notebook unchanged.\n",
    "TEST_QUERIES = [\n",
    "    \"forensic engineer wind damage expert witness Florida property insurance site:linkedin.com\",\n",
    "    \"building envelope consultant moisture intrusion expert witness Florida site:linkedin.com\",\n",
    "    \"forensic accountant business interruption insurance claim expert witness site:linkedin.com\",\n",
    "    \"meteorologist hurricane wind field expert witness litigation site:linkedin.com\",\n",
    "    \"fire origin and cause investigator expert witness insurance litigation site:linkedin.com\",\n",
    "    \"policyholder attorney bad faith property insurance Florida site:linkedin.com\",\n",
    "    \"Texas hail damage property insurance attorney appraisal dispute site:linkedin.com\",\n",
    "    \"insurance appraisal umpire property claims Florida site:linkedin.com\",\n",
    "    \"licensed public adjuster large loss hurricane Florida commercial property site:linkedin.com\",\n",
    "    \"Xactimate trainer estimator large loss consultant site:linkedin.com\",\n",
    "    \"claims consultant catastrophe response litigation support property insurance site:linkedin.com\",\n",
    "    \"insurance coverage expert witness former adjuster property claims site:linkedin.com\",\n",
    "    \"water mitigation IICRC expert witness insurance dispute site:linkedin.com\",\n",
    "    \"roofing consultant wind uplift tile roof expert witness Florida site:linkedin.com\",\n",
    "    \"civil engineer structural damage assessment hurricane expert witness site:linkedin.com\",\n",
    "]\n",
    "\n",
    "RELEVANCE_KEYWORDS = [\n",
    "    \"expert witness\",\n",
    "    \"forensic\",\n",
    "    \"insurance\",\n",
    "    \"appraisal\",\n",
    "    \"adjuster\",\n",
    "    \"coverage\",\n",
    "    \"litigation\",\n",
    "    \"catastrophe\",\n",
    "]\n",
    "\n",
    "batch_rows = []\n",
    "for query in TEST_QUERIES:\n",
    "    resp, meta = exa_search_people(query, num_results=CONFIG[\"num_results\"])\n",
    "    results = resp.get(\"results\", []) if isinstance(resp, dict) else []\n",
    "    top = results[0] if results else {}\n",
    "\n",
    "    top_n = results[: int(CONFIG[\"num_results\"])]\n",
    "    linkedin_present = any(\"linkedin.com\" in str(r.get(\"url\") or \"\").lower() for r in top_n)\n",
    "\n",
    "    relevance_text = []\n",
    "    for r in top_n:\n",
    "        parts = [str(r.get(\"title\") or \"\")]\n",
    "        highlights = r.get(\"highlights\")\n",
    "        if isinstance(highlights, list):\n",
    "            parts.extend(str(x) for x in highlights)\n",
    "        text = r.get(\"text\")\n",
    "        if isinstance(text, str):\n",
    "            parts.append(text[:200])\n",
    "        relevance_text.append(\" \".join(parts).lower())\n",
    "    relevance_keywords_present = any(any(k in blob for k in RELEVANCE_KEYWORDS) for blob in relevance_text)\n",
    "\n",
    "    batch_rows.append(\n",
    "        {\n",
    "            \"query\": query,\n",
    "            \"cache_hit\": meta.cache_hit,\n",
    "            \"est_cost_usd\": meta.estimated_cost_usd,\n",
    "            \"actual_cost_usd\": meta.actual_cost_usd,\n",
    "            \"top_title\": redact_text(top.get(\"title\")) if isinstance(top, dict) else None,\n",
    "            \"top_url\": top.get(\"url\") if isinstance(top, dict) else None,\n",
    "            \"top_preview\": extract_preview(top, max_chars=220) if isinstance(top, dict) else \"\",\n",
    "            \"linkedin_present\": linkedin_present,\n",
    "            \"relevance_keywords_present\": relevance_keywords_present,\n",
    "            \"result_count\": len(results),\n",
    "        }\n",
    "    )\n",
    "\n",
    "df_batch = pd.DataFrame(batch_rows)\n",
    "print(f\"Batch queries executed: {len(df_batch)}\")\n",
    "df_batch\n"
   ],
   "id": "cell06"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 - Summary table + qualitative notes\n",
    "\n",
    "df_ledger = ledger_summary()\n",
    "summary = spend_so_far()\n",
    "\n",
    "print(f\"Request count: {summary['request_count']}\")\n",
    "print(f\"Cache hits vs uncached calls: {summary['cache_hits']} vs {summary['uncached_calls']}\")\n",
    "print(f\"Spent so far (USD): ${summary['spent_usd']:.4f}\")\n",
    "print(f\"Avg cost per uncached query (USD): ${summary['avg_cost_per_uncached_query']:.4f}\")\n",
    "\n",
    "summary_table = pd.DataFrame([summary])\n",
    "print(\"\\nSummary table\")\n",
    "try:\n",
    "    print(summary_table.to_markdown(index=False))\n",
    "except Exception:\n",
    "    print(summary_table)\n",
    "\n",
    "qualitative_notes: List[str] = []\n",
    "if not df_batch.empty:\n",
    "    relevance_rate = float(df_batch[\"relevance_keywords_present\"].mean())\n",
    "    linkedin_rate = float(df_batch[\"linkedin_present\"].mean())\n",
    "    avg_results = float(df_batch[\"result_count\"].mean())\n",
    "    qualitative_notes.append(f\"Observed relevance-keyword signal rate: {relevance_rate:.0%} across batch queries.\")\n",
    "    qualitative_notes.append(f\"LinkedIn/public professional profile signal appeared in {linkedin_rate:.0%} of queries.\")\n",
    "    qualitative_notes.append(f\"Average result count returned per query: {avg_results:.1f} (configured num_results={CONFIG['num_results']}).\")\n",
    "else:\n",
    "    qualitative_notes.append(\"No batch results yet. Run Cell 6 first.\")\n",
    "\n",
    "if CONFIG[\"use_text\"]:\n",
    "    qualitative_notes.append(\"Text is enabled: higher evidence quality, higher cost. Consider disabling for initial screening.\")\n",
    "else:\n",
    "    qualitative_notes.append(\"Text is disabled: cheaper baseline. Highlights should usually be enough for triage.\")\n",
    "\n",
    "if CONFIG[\"use_summary\"]:\n",
    "    qualitative_notes.append(\"Summary is enabled: validate value before scaling because it adds per-result cost.\")\n",
    "else:\n",
    "    qualitative_notes.append(\"Summary is disabled (recommended for cost-sensitive baseline testing).\")\n",
    "\n",
    "if EXA_SMOKE_NO_NETWORK:\n",
    "    qualitative_notes.append(\"Smoke mode is on: results are mocked and costs are zero; use a real API key for live quality/cost evaluation.\")\n",
    "\n",
    "print(\"\\nQualitative notes\")\n",
    "for note in qualitative_notes:\n",
    "    print(\"-\", note)\n",
    "\n",
    "review_cols = [\n",
    "    \"query\",\n",
    "    \"top_title\",\n",
    "    \"top_url\",\n",
    "    \"top_preview\",\n",
    "    \"linkedin_present\",\n",
    "    \"relevance_keywords_present\",\n",
    "    \"cache_hit\",\n",
    "    \"actual_cost_usd\",\n",
    "    \"est_cost_usd\",\n",
    "]\n",
    "df_batch[review_cols]\n"
   ],
   "id": "cell07"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 - Cost estimate + projections\n",
    "\n",
    "observed_avg_uncached = float(summary.get(\"avg_cost_per_uncached_query\", 0.0))\n",
    "observed_spent = float(summary.get(\"spent_usd\", 0.0))\n",
    "\n",
    "# If a full rerun is cached (avg becomes 0), fall back to a config-based estimate so projections stay useful.\n",
    "projection_basis = \"observed_avg_uncached\"\n",
    "projection_unit_cost = observed_avg_uncached\n",
    "if projection_unit_cost <= 0:\n",
    "    baseline_payload = build_exa_payload(\"baseline projection query\", num_results=CONFIG[\"num_results\"])\n",
    "    projection_unit_cost = _estimate_cost_from_pricing(baseline_payload, int(baseline_payload[\"numResults\"]))\n",
    "    projection_basis = \"estimated_from_current_config\"\n",
    "\n",
    "projections = {\n",
    "    \"projection_basis\": projection_basis,\n",
    "    \"unit_cost_usd\": round(projection_unit_cost, 6),\n",
    "    \"projected_100_queries_usd\": round(projection_unit_cost * 100, 4),\n",
    "    \"projected_1000_queries_usd\": round(projection_unit_cost * 1000, 4),\n",
    "    \"projected_10000_queries_usd\": round(projection_unit_cost * 10000, 4),\n",
    "}\n",
    "\n",
    "print(f\"Spent so far (USD): ${observed_spent:.4f}\")\n",
    "print(f\"Projection basis: {projection_basis}\")\n",
    "print(f\"Projected cost for 100 queries:   ${projections['projected_100_queries_usd']:.4f}\")\n",
    "print(f\"Projected cost for 1,000 queries: ${projections['projected_1000_queries_usd']:.4f}\")\n",
    "print(f\"Projected cost for 10,000 queries:${projections['projected_10000_queries_usd']:.4f}\")\n",
    "\n",
    "cost_projection_table = pd.DataFrame([projections])\n",
    "cost_projection_table\n"
   ],
   "id": "cell08"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9 - Decision rubric + recommendation integration points\n",
    "\n",
    "RUBRIC = {\n",
    "    \"Search quality (people relevance)\": \"Do results return the right categories of professionals for CAT-loss / claim-dispute work?\",\n",
    "    \"Coverage\": \"Does it find credible candidates beyond obvious first-page names?\",\n",
    "    \"Evidence quality\": \"Are highlights/snippets sufficient for triage without pulling full page text?\",\n",
    "    \"Latency\": \"Is response time acceptable for interactive analyst workflows?\",\n",
    "    \"Cost\": \"Is unit cost predictable and within your budget at projected volumes?\",\n",
    "    \"Safety/compliance\": \"Can outputs stay public/professional only (no doxxing/contact harvesting)?\",\n",
    "    \"Repeatability\": \"Do cached reruns reliably reproduce demo/eval behavior without rebilling?\",\n",
    "}\n",
    "\n",
    "print(\"Decision rubric (mark manually after review)\")\n",
    "for criterion, prompt in RUBRIC.items():\n",
    "    print(f\"- [ ] {criterion}: {prompt}\")\n",
    "\n",
    "\n",
    "def recommendation(summary_metrics: Dict[str, Any], batch_df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    if batch_df.empty:\n",
    "        relevance_rate = 0.0\n",
    "        linkedin_rate = 0.0\n",
    "    else:\n",
    "        relevance_rate = float(batch_df[\"relevance_keywords_present\"].mean())\n",
    "        linkedin_rate = float(batch_df[\"linkedin_present\"].mean())\n",
    "\n",
    "    avg_cost = float(summary_metrics.get(\"avg_cost_per_uncached_query\", 0.0))\n",
    "\n",
    "    headline = \"Integrate only for scoped workflows\"\n",
    "    if relevance_rate >= 0.70 and (avg_cost <= 0.02 or EXA_SMOKE_NO_NETWORK):\n",
    "        headline = \"Integrate (with human review and budget guards)\"\n",
    "    if relevance_rate < 0.50 or avg_cost > 0.05:\n",
    "        headline = \"Do not integrate at current settings\"\n",
    "\n",
    "    return {\n",
    "        \"headline_recommendation\": headline,\n",
    "        \"observed_relevance_rate\": round(relevance_rate, 3),\n",
    "        \"observed_linkedin_rate\": round(linkedin_rate, 3),\n",
    "        \"avg_cost_per_uncached_query_usd\": round(avg_cost, 4),\n",
    "        \"budget_cap_usd\": float(CONFIG[\"budget_cap_usd\"]),\n",
    "        \"safety_guardrails\": [\n",
    "            \"Public/professional info only\",\n",
    "            \"No address hunting or contact harvesting\",\n",
    "            \"Keep redaction enabled for displayed snippets\",\n",
    "            \"Human review required before any operational use\",\n",
    "        ],\n",
    "        \"integration_points\": [\n",
    "            {\n",
    "                \"workflow\": \"Expert / professional discovery\",\n",
    "                \"value\": \"Find candidate experts and collect source URLs plus short relevance snippets for analyst review.\",\n",
    "                \"safe_pattern\": \"Query by role + peril + jurisdiction; store only title/url/highlights unless deeper review is justified.\",\n",
    "            },\n",
    "            {\n",
    "                \"workflow\": \"Consultant / witness context enrichment\",\n",
    "                \"value\": \"When a report names a professional, pull public bios/publications for litigation context.\",\n",
    "                \"safe_pattern\": \"Search by name + role + insurance/litigation terms; do not harvest private contact data.\",\n",
    "            },\n",
    "            {\n",
    "                \"workflow\": \"Claim dispute research triage\",\n",
    "                \"value\": \"Quickly identify likely relevant disciplines (forensic engineer, meteorologist, accountant, etc.).\",\n",
    "                \"safe_pattern\": \"Use highlights-on/text-off default, then selectively expand only high-value results.\",\n",
    "            },\n",
    "        ],\n",
    "        \"next_tuning_moves\": [\n",
    "            \"Keep highlights on, text off, summary off, num_results=5 for baseline cost testing.\",\n",
    "            \"Try include_domains for stricter professional-source targeting if relevance is noisy.\",\n",
    "            \"Enable text/summary only for a second-pass workflow on shortlisted results.\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "rec = recommendation(summary, df_batch)\n",
    "print(\"\\nRecommendation output\")\n",
    "print(json.dumps(rec, indent=2))\n"
   ],
   "id": "cell09"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
